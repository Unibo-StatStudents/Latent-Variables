---
title: "Cheat Sheet"
output: github_document
author: "Sebastian Veuskens"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminaries 
## Load libraries 

```{r}
library(lavaan)         # For Holzinger data 
library(GPArotation)    # For Factor rotations 
library(ltm)            # For Abortion data and Item response theory 
library(poLCA)          # For latent class analysis 
```

## Load data 

```{r}
data(HolzingerSwineford1939)
data(Abortion)
data(carcinoma)
data(Mobility)
```

# Exploratory Analysis 

```{r}
#################
#### Modify #####
#################
data <- Abortion 
#################
head(data)
dim(data) 
summary(data)
str(data)
heatmap(cor(data), scale='none')
```

# Normal linear factor Model

```{r}
#################
#### Modify #####
#################
data <- HolzingerSwineford1939[,c("x1","x2","x3","x4","x5","x6","x7","x8","x9")]
# cov_mat <- as.matrix()                        # Alternative for data if only covariance/correlation matrix is given
# n_obs <-                                      # Number of observations, only needed if cov_mat is used instead of data 
formula_nlf <- ~x1+x2+x3+x4+x5+x6+x7+x8+x9      # Variables to include into the analysis (response variables)
n_facs <- c(1, 2, 3)                            # Vector of numbers of factors to be used 
cut_off <- 0.2                                  # Correlation threshold onto which not to include in the factor loadings 
n_items <- ncol(data)                           # The number of items p included into the analysis 
fac_score_method <- "Bartlett"                  # Alternative: "regression" -> Used method to calculate individual scores 
formula_cfa <- 'visual =~ x1 + x2 + x3
                textual =~ x4 + x5 + x6
                speed =~ x7 + x8 + x9'          # Connect factors and manifest variables manually (create your own loadings matrix) 
#################

## Correlation matrix 
cov_mat <- cor(data)
cov_mat 

## ML-estimation of factor model 
chi_sq <- list() 
df <- list() 
p_value <- list() 
fac_models <- list() 
for (n_fac in n_facs) {
    fac_model_cand <- factanal(x = formula_nlf, factors = n_fac, data = data, scores = fac_score_method)
    # fac_model_cand <- factanal(x = formula_nlf, factors = n_fac, covmat = cov_mat, n.obs=n_obs, rotation='none')
    fac_models[[n_fac]] <- fac_model_cand 
    chi_sq[[n_fac]] <- fac_model_cand$STATISTIC
    df[[n_fac]] <- fac_model_cand$dof 
    p_value[[n_fac]] <- fac_model_cand$PVAL
}

best_n_fac <- n_facs[which.max(unlist(p_value))]
fac_model <- fac_models[[best_n_fac]]

## Model statistics 

chi_sq[[best_n_fac]]
df[[best_n_fac]]
p_value[[best_n_fac]]

## Model evaluation 
# Factor loadings of the model 
loadings(fac_model)
print(fac_model, cutoff = cut_off)

# Communalities -> Percentage of variability explained by factors for each manifest variable/item/indicator (x_i) 
commun <- rowSums(loadings(fac_model)^2)        # Alternatively: commun <- 1 - fac_model$uniquenesses 
commun
# Total explained variance 
perc_var <- sum(commun) / n_items 
perc_var

#### Results #### 
## Individual scores  
ind_scores <- fac_model$scores 
## Reproduced correlation matrix 
rep_corr <- loadings(fac_model) %*% t(loadings(fac_model)) 
rep_corr
## Discrepancy matrix 
round(cov_mat - rep_corr, 3)          

## Factor rotations 
# Orthogonal rotations 
Varimax(loadings(fac_model))                    # Factors with few large and many near zero loadings 
quartimax(loadings(fac_model))                  # Aims at the correspondence between observed and latent factors 
# Oblique rotations 
promax(loadings(fac_model))                     # Simple structure with low correlation between factors 
oblimin(loadings(fac_model))                    # Similar to Varimax (few large and many zero), but oblique rotation 

# Display loadings for confimatory analysis formula 
heatmap(Varimax(loadings(fac_model))$loadings, scale='none')
heatmap(quartimax(loadings(fac_model))$loadings, scale='none')
heatmap(promax(loadings(fac_model))$loadings, scale='none')
heatmap(oblimin(loadings(fac_model))$loadings, scale='none')

#### Confirmatory analysis 
# std.lv (variance standardization) -> Standard loading variances, means that the variance for all factors is set to one 
# The default alternative is the marker method 
# orthogonal indicates the Covariance matrix of the factors: If true, all factors will be uncorrelated 
# orthogonal decreases the number of free model parameters 
cfa_model <- cfa(formula_cfa, data = data, std.lv = FALSE, orthogonal = FALSE) 
# cfa_model <- cfa(formula_cfa, sample.cov=cov_mat, sample.nobs=n_obs, std.lv = FALSE) 
summary(cfa_model, fit.measures = TRUE)
```

# Item response theory/Response function approach 

The general IRT (Item Response Theory) form for the parametrization is:
$log(\frac{\pi_i}{1 - \pi_i}) = \beta_{0i} + \beta_{1i}z_1 + \beta_{2i}z_2$ 

With IRT.param = TRUE 
$\rightarrow log(\frac{\pi_i}{1 - \pi_i}) = \beta_{1i} (z - \beta^*_{0i})$ 

The discrimination parameter for item $i$ and factor $j$ is $\alpha_{ij}$ 

**Summary**
Use IRT.param = TRUE -> use beta parametrization 
Use IRT.param = FALSE -> use alpha parametrization 

**Component scores**
The component scores for the $j$'s factor are in general are:
$sum_{i=1}^p \alpha_{ij} x_i$
Thus, for latent trait models it is just the sum of $\alpha_{ij}$'s for which $x_i=1$. 

```{r}
#################
#### Modify #####
#################
data <- Abortion 
formula_ltm <- data~z1 
q <- 1                                                  # Specify the number of factors used in the model 
p <- ncol(data)
i <- 1                                                  # Item to investigate further
j <- 1                                                  # Factor to investigate further 
missing_patterns <- rbind(c(1, 0, 0, 1), c(1, 0, 1, 0))
#################

#### For dichotomous and polytomous (categorical) data (?), describes each item with frequencies #### 
dsc <- descript(data)
dsc$perc 
dsc$items 
## P-values that test independence between two items (H_0 == independent)
# This is Goodness of fit (see Latent trait models, marginal frequencies)
dsc$pw.ass              # Applies the Chi-squared test and second-order margins to check for positive correlation in the data (not in the model!)
plot(dsc)

#### Latent trait model ####
## Computation 
ltm_model <- ltm(formula_ltm, IRT.param = TRUE)         # IRT.param - NEGATIVE difficulty parameter -> indicates which parametrization to use: If TRUE, low value means low difficulty (easy)
ltm_model_rip <- ltm(formula_ltm, IRT.param=FALSE)
# From IRT.param = TRUE to IRT.param = False: 
coefs <- summary(ltm_model)$coefficients
-coefs[i, j] * coefs[i + p, j]                          # == summary(ltm_model_rip)$coefficients[i, j]
# Alternative: Rasch model -> All \beta_{i1} are equal to 1, constraint does this (p+1 indicates discrimination parameters)
# ltm_model <- rasch(data, IRT.param=TRUE, constraint=cbind(p+1, 1))

## Test models 
ltm_model_no_con <- rasch(data, IRT.param=TRUE)
ltm_model_con <- rasch(data, IRT.param=TRUE, constraint=cbind(p+1, 1))
anova(ltm_model_con, ltm_model_no_con)                  # The lower AIC and BIC, the better 

## Results 
summary(ltm_model)                                      # Gives estimates for difficulties and discrimination parameters (beta_i0 and beta_i1)                      
coef(ltm_model, prob=TRUE, order=TRUE)                  # Same as summary + probabilities of median individual 
summary(ltm_model)$coefficients[i, j]                   # Access the estimate values 

alpha_zero <- ltm_model_rip$coeff[,1]
alpha <- ltm_model$coeff[,2]                            # Discrimination parameters 
# st_alpha <- alpha / sqrt(1 + sum(alpha^2))            # Somehow correlation between item x_i and latent variable y_j 
st_alpha <- alpha / sqrt(1 + alpha^2)                   # Somehow correlation between item x_i and latent variable y_j 
st_alpha 

## Visualize 
plot(ltm_model, legend=TRUE, cx = "bottomright", xlab="Attitude toward/Difficulty", lwd=3, cex.main=1.5, cex.lab = 1.3, cex=1.1)

#### Goodness of fit of model #### 
fit_model <- fitted(ltm_model)
fit_model

expected <- fit_model[,ncol(data) + 1]                  # Expected frequencies are in last column in fit_model, before are the items 
observed <- ltm_model$pattern$obs                       # The frequencies observed in the data 
patterns <- ltm_model$pattern$X                         # Each pattern that was observed 
n_pat <- nrow(patterns) 

cbind(patterns, observed, expected)

## Degrees of freedom 
df <- n_pat - p * (q+1) - 1
df
## Chi-square test for complete patterns 
chi_sq_ltm <- sum((expected - observed)^2 / expected)
pvalue_chisq <- 1 - pchisq(chi_sq_ltm, df)
pvalue_chisq

## Likelihood ratio test 
lr <- 2 * sum(observed * log(observed / expected))      # observed here is same as n in slides               
pvalue_lr <- 1 - pchisq(lr, df)
pvalue_lr

## Bootstrap for Rasch model 
pvalue_boot <- GoF.rasch(ltm_model, B=200)
pvalue_boot$Tobs
pvalue_boot

## Check two and three-way margins 
margins(ltm_model)
margins(ltm_model, type='three-way', nprint=2)          # nprint determines here the number of marginals to print with the largest chi-squared residuals value 

#### Factor scores #### 
## Scores for observed response patterns 
factor_scores <- factor.scores(ltm_model, method='EAP') # EAP: expected apriori pattern - Exp means expected, not exponent or so 
factor_scores                                           # I think low scores for z1 etc. mean low ability/agreement for these individiual with that certain pattern 
plot(factor_scores$score.dat$z1)

## Total number of ones for each pattern 
resp_pattern <- factor_scores$score.dat[,1:p]
tot_scores <- apply(resp_pattern, 1, sum)                # Number of positive responses per pattern (sum of ones)  
tot_scores  
plot(tot_scores, factor_scores$score.dat$z1)

## Scores for components 
comp <- factor.scores(ltm_model, method='Component')    # Get the components (z1), which is (for latent trait): sum_{i=1}^p \alpha_{ij} x_i
comp_scores <- comp$score.dat[,p+3]

## Check ordering, according to total score and factor/component scores ordering (which are equivalent)
tab <- cbind(factor_scores$score.dat, comp_scores, tot_scores)
round(tab, 3)
round(tab[order(tot_scores),], 3)

## Compute scores for missing patterns 
factor.scores(ltm_model, resp.pattern=missing_patterns)
```

# Latent class analysis 

```{r}
#################
#### Modify #####
#################
data <- carcinoma                               # Has to be dichotomous - only one and two as values
p <- ncol(data)
formula_lca <- cbind(A, B, C, D, E, F, G) ~ 1   # Formula of which columns/variables to include from the data 
n_classes <- c(2, 3, 4)                                    # Number of latent classes 
h <- 1                                          # Observation of interest (specific pattern) 
j <- 1                                          # Class of interest -> Absolute value, not relative position here! 
outcome <- c(2, 2, 2, 2, 2, 2, 2)               # For joint and posterior probability estimation 
pattern_interest <- c(1, 1, 1, 1, 1, 1, 1)      # Pattern of interest  
h <- 1                                          # Observation of interest 
#################

## Computation 
lca_models <- list() 
for (n_class in n_classes) {
    cur_model <- poLCA(formula_lca, data, nclass=n_class, nrep=10, verbose=FALSE)
    lca_models[[n_class]] = cur_model
}

freq_estim <- data.frame(lca_models[[n_classes[1]]]$predcell[1:(p + 2)])
for (n_class in n_classes[2:length(n_classes)]) {
    freq_estim = cbind(freq_estim, lca_models[[n_class]]$predcell[p+2])
}
freq_estim

## Results 
llik <- unlist(purrr::map(lca_models, 'llik'))
npar <- unlist(purrr::map(lca_models, 'npar'))
Gsq <- round(unlist(purrr::map(lca_models, 'Gsq')), 3)
Chisq <- unlist(purrr::map(lca_models, 'Chisq'))
df <- unlist(purrr::map(lca_models, 'resid.df'))
aic <- unlist(purrr::map(lca_models, 'aic'))
bic <- unlist(purrr::map(lca_models, 'bic'))
p_value <- round(1 - pchisq(Chisq, df), 3)

# Choose the one with lowest AIC/BIC 
summ <- data.frame(n_classes, llik, npar, Gsq, Chisq, df, p_value, aic, bic)
summ

which.min(aic) == which.min(bic)
n_class_best <- n_classes[which.min(bic)]
best_lca_model <- lca_models[[n_class_best]]
# Patterns and observed vs. expected frequencies 
best_lca_model$predcell 
# Prior for class (y)
round(best_lca_model$P, 3)
# Conditional on class (y): g(x_i|y = j) for each j and item i 
lapply(best_lca_model$probs, round, 3)

## Reorder to receive comparable results (order by the prior for y -> from lowest to highest)
probs <- best_lca_model$P 
probs_start <- best_lca_model$probs.start
new_probs_start <- poLCA.reorder(probs_start, order(probs)) # Order by final probabilities received 
best_lca_ord <- poLCA(formula_lca, data, nclass=n_class_best, probs.start=new_probs_start, verbose=FALSE)
round(best_lca_ord$P, 3)
lapply(best_lca_ord$probs, round, 3)

# Probability estimation of agreement between variables for 1 outcome (positive or negative) per class (jointp_outcome) and total (sum(jointp_outcome))
# In other words, joint probability of being in class j and having only 1 or 0 as outcome 
# If I want another class, change outcome -> indicates the position of the outcome, not the absolute value! 
length(outcome) == p
jointp_outcome <- list() 
for (class in 1:n_class_best) {
    current <- probs[class]
    for (item in 1:p) {
        current <- current * best_lca_ord$probs[[item]][class, outcome[item]]
    }
    jointp_outcome[[class]] <- current 
}
jointp_outcome <- unlist(jointp_outcome)
# Joint probabilities f(x, y_j) for each j 
round(jointp_outcome, 5)
# Just probability of receiving observation outcome in general (independent of class) f(x) 
sum(jointp_outcome)
# poLCA.predcell(best_lca_ord, y=outcome)
# Posterior probabilities 
round(jointp_outcome / sum(jointp_outcome), 5)
# poLCA.posterior(best_lca_ord, outcome)

# Compute expected number of cases with only zeros 
round(sum(jointp_outcome) * best_lca_ord$N, 3)
# prob_zero_class1 <- probs[1] * probs[]

## Posterior probabilities for classes 
best_lca_ord$posterior                              # Probability of observation h (rows) to be in class j (columns)
round(best_lca_ord$posterior[h,], 3)                # Class probabilities for observation h 
# Predicted classes 
best_lca_ord$predclass 
table(best_lca_ord$predclass)
unique(data[best_lca_ord$predclass==j,])            # All (unique) patterns that are predicted to be class j in the data  
# Get observations of class j 
data[best_lca_ord$predclass == j,]

# Compute posterior probabilities and class-independent probabilities for (missing) patterns 
poLCA.posterior(best_lca_ord, pattern_interest)     # y_j|x for each j 
poLCA.predcell(best_lca_ord, y=pattern_interest)    # f(x) = sum_j(f(x, y_j)) 
```
```{r}

```
```{r}

```
```{r}

```
```{r}

```
```{r}

```
```{r}

```